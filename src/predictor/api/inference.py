#!/usr/bin/env python3
"""
Tennis Ball Detection Inference API
===================================

ğŸ¾ ãƒ†ãƒ‹ã‚¹ãƒœãƒ¼ãƒ«æ¤œå‡ºæ¨è«–çµ±åˆã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ãƒ†ãƒ‹ã‚¹å‹•ç”»ã‹ã‚‰ã®ãƒœãƒ¼ãƒ«æ¤œå‡ºã€å¯è¦–åŒ–ã€çµ±è¨ˆå‡ºåŠ›ã¾ã§ã‚’
ãƒ¯ãƒ³ã‚¹ãƒˆãƒƒãƒ—ã§å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ã™ã€‚

Features:
- ğŸ¤– è¤‡æ•°æ¤œå‡ºå™¨å¯¾å¿œ (LiteTrackNet, WASB-SBDT)
- ğŸš€ ä¸¦åˆ—å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- ğŸ¨ é«˜å“è³ªå¯è¦–åŒ–
- ğŸ“Š çµ±è¨ˆæƒ…å ±å‡ºåŠ›
- âš¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã€œãƒãƒƒãƒå‡¦ç†

Usage:
    python -m src.predictor.api.inference \
        --config-path ../../configs/infer \
        --config-name inference \
        io.video=input.mp4 \
        io.output=output.mp4 \
        model.model_path=checkpoints/model.ckpt \
        model=lite_tracknet \
        pipeline=high_performance

Examples:
    # åŸºæœ¬å®Ÿè¡Œ
    python -m src.predictor.api.inference \
        --config-path ../../configs/infer \
        --config-name inference \
        io.video=tennis_match.mp4 \
        io.output=annotated_match.mp4 \
        model.model_path=checkpoints/lite_tracknet.ckpt

    # é«˜æ€§èƒ½ä¸¦åˆ—å‡¦ç†
    python -m src.predictor.api.inference \
        --config-path ../../configs/infer \
        --config-name inference \
        io.video=long_match.mp4 \
        io.output=result.mp4 \
        model.model_path=models/wasb_sbdt.pth \
        model=wasb_sbdt \
        pipeline=high_performance \
        pipeline.batch_size=16 \
        pipeline.num_workers=8

    # ã‚«ã‚¹ã‚¿ãƒ å¯è¦–åŒ–
    python -m src.predictor.api.inference \
        --config-path ../../configs/infer \
        --config-name inference \
        io.video=input.mp4 \
        io.output=stylized.mp4 \
        model.model_path=model.ckpt \
        visualization=enhanced \
        io.stats_output=stats.json
"""

import copy
import json
import logging
import os
import sys
import time
from pathlib import Path
from typing import Dict, Any, Optional

import hydra
import torch
from omegaconf import DictConfig, OmegaConf

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from src.predictor import (
    VideoPipeline, 
    create_ball_detector,
    VisualizationConfig,
    HIGH_PERFORMANCE_CONFIG,
    MEMORY_EFFICIENT_CONFIG,
    REALTIME_CONFIG,
    DEBUG_CONFIG
)
from src.predictor.pipeline.config import PipelineConfig


def get_pipeline_config(cfg: DictConfig) -> PipelineConfig:
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­å®šå–å¾—"""
    try:
        # ãƒ™ãƒ¼ã‚¹è¨­å®šé¸æŠ
        if cfg.pipeline.type == 'high_performance':
            base_config = HIGH_PERFORMANCE_CONFIG
        elif cfg.pipeline.type == 'memory_efficient':
            base_config = MEMORY_EFFICIENT_CONFIG
        elif cfg.pipeline.type == 'realtime':
            base_config = REALTIME_CONFIG
        elif cfg.pipeline.type == 'debug':
            base_config = DEBUG_CONFIG
        else:
            raise ValueError(f"Unknown pipeline type: {cfg.pipeline.type}")
        
        # PipelineConfigã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚³ãƒ”ãƒ¼
        config = copy.copy(base_config)
        
        # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã®å€¤ã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’æä¾›ï¼‰
        batch_size = getattr(cfg.pipeline, 'batch_size', 8)
        num_workers = getattr(cfg.pipeline, 'num_workers', 4)
        queue_size = getattr(cfg.pipeline, 'queue_size', 100)
        
        # PipelineConfigã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°
        if hasattr(config, 'gpu_batch_size'):
            config.gpu_batch_size = batch_size
        if hasattr(config, 'num_workers'):
            config.num_workers = num_workers
        if hasattr(config, 'frame_buffer_size'):
            config.frame_buffer_size = queue_size
            
        # PipelineConfigã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç›´æ¥è¿”ã™
        return config
    
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
        logging.warning(f"Pipeline config error: {e}. Using default PipelineConfig.")
        fallback_config = PipelineConfig()
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
        fallback_config.gpu_batch_size = getattr(cfg.pipeline, 'batch_size', 8)
        fallback_config.num_workers = getattr(cfg.pipeline, 'num_workers', 4)
        fallback_config.frame_buffer_size = getattr(cfg.pipeline, 'queue_size', 100)
        return fallback_config


def get_detector_config(cfg: DictConfig) -> Dict[str, Any]:
    """æ¤œå‡ºå™¨è¨­å®šå–å¾—"""
    # ãƒ‡ãƒã‚¤ã‚¹è‡ªå‹•æ¤œå‡º
    if cfg.model.device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device = cfg.model.device
    
    # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—è‡ªå‹•åˆ¤å®š
    if cfg.model.type == 'auto':
        if cfg.model.model_path.endswith('.ckpt'):
            model_type = 'lite_tracknet'
        elif cfg.model.model_path.endswith('.pth'):
            model_type = 'wasb_sbdt'
        else:
            raise ValueError(f"Cannot auto-detect model type from: {cfg.model.model_path}")
    else:
        model_type = cfg.model.type
    
    config = {
        'model_path': cfg.model.model_path,
        'model_type': model_type,
        'device': device,
    }
    
    if cfg.model.config_path:
        config['config_path'] = cfg.model.config_path
        
    return config


def get_visualization_config(cfg: DictConfig) -> VisualizationConfig:
    """å¯è¦–åŒ–è¨­å®šå–å¾—"""
    return VisualizationConfig(
        ball_radius=cfg.visualization.ball_radius,
        trajectory_length=cfg.visualization.trajectory_length,
        enable_smoothing=cfg.visualization.enable_smoothing,
        enable_prediction=cfg.visualization.enable_prediction,
        confidence_threshold=cfg.visualization.confidence_threshold
    )


def calculate_statistics(result: Dict[str, Any]) -> Dict[str, Any]:
    """çµ±è¨ˆæƒ…å ±è¨ˆç®—"""
    stats = {
        'processing_info': {
            'total_frames': result.get('total_frames', 0),
            'processed_frames': result.get('processed_frames', 0),
            'processing_time': result.get('processing_time', 0.0),
            'average_fps': result.get('average_fps', 0.0),
        },
        'detection_stats': {
            'total_detections': 0,
            'frames_with_detection': 0,
            'average_confidence': 0.0,
            'detection_rate': 0.0,
        },
        'performance_stats': {
            'gpu_utilization': result.get('gpu_utilization', 0.0),
            'memory_usage': result.get('memory_usage', 0.0),
            'queue_efficiency': result.get('queue_efficiency', 0.0),
        }
    }
    
    # æ¤œå‡ºçµ±è¨ˆè¨ˆç®—
    detections = result.get('detections', {})
    if detections:
        total_detections = sum(len(det_list) for det_list in detections.values())
        frames_with_detection = sum(1 for det_list in detections.values() if det_list)
        
        all_confidences = []
        for det_list in detections.values():
            all_confidences.extend([det[2] for det in det_list if len(det) > 2])
        
        stats['detection_stats'].update({
            'total_detections': total_detections,
            'frames_with_detection': frames_with_detection,
            'average_confidence': sum(all_confidences) / len(all_confidences) if all_confidences else 0.0,
            'detection_rate': frames_with_detection / len(detections) if detections else 0.0,
        })
    
    return stats


def save_statistics(stats: Dict[str, Any], output_path: str):
    """çµ±è¨ˆæƒ…å ±ä¿å­˜"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    logging.info(f"çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜: {output_path}")


def display_progress(result, progress_interval: float):
    """é€²æ—è¡¨ç¤º"""
    while not result.is_completed():
        progress = result.get_progress()
        current_fps = result.get_current_fps()
        logging.info(f"é€²æ—: {progress*100:.1f}% | ç¾åœ¨ã®FPS: {current_fps:.1f}")
        time.sleep(progress_interval)


def validate_config(cfg: DictConfig) -> None:
    """è¨­å®šæ¤œè¨¼"""
    required_fields = [
        ('io.video', 'input video file'),
        ('io.output', 'output video file'),
        ('model.model_path', 'model file path')
    ]
    
    for field_path, description in required_fields:
        field_value = cfg
        for key in field_path.split('.'):
            field_value = getattr(field_value, key, None)
        if not field_value:
            raise ValueError(f"{field_path} ({description}) is required but not provided")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
    if not os.path.exists(cfg.io.video):
        raise FileNotFoundError(f"Input video not found: {cfg.io.video}")
    
    if not os.path.exists(cfg.model.model_path):
        raise FileNotFoundError(f"Model file not found: {cfg.model.model_path}")


@hydra.main(version_base=None, config_path="../../configs/infer", config_name="inference")
def main(cfg: DictConfig) -> None:
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ"""
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=getattr(logging, cfg.system.log_level),
        format='[%(levelname)s] %(message)s'
    )
    logging.info("ğŸ¾ Tennis Ball Detection Inference System é–‹å§‹")
    
    try:
        # è¨­å®šæ¤œè¨¼
        validate_config(cfg)
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir = os.path.dirname(cfg.io.output)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        # è¨­å®šå–å¾—
        pipeline_config = get_pipeline_config(cfg)
        detector_config = get_detector_config(cfg)
        vis_config = get_visualization_config(cfg)
        
        logging.info(f"è¨­å®š: {OmegaConf.to_yaml(cfg)}")
        logging.info(f"å…¥åŠ›å‹•ç”»: {cfg.io.video}")
        logging.info(f"å‡ºåŠ›å‹•ç”»: {cfg.io.output}")
        logging.info(f"ãƒ¢ãƒ‡ãƒ«: {detector_config['model_type']} ({cfg.model.model_path})")
        logging.info(f"ãƒ‡ãƒã‚¤ã‚¹: {detector_config['device']}")
        logging.info(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­å®š: {cfg.pipeline.type}")
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
        pipeline = VideoPipeline(pipeline_config)
        
        # å‡¦ç†å®Ÿè¡Œ
        start_time = time.time()
        
        if cfg.system.async_processing:
            # éåŒæœŸå‡¦ç†
            logging.info("éåŒæœŸå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­...")
            result = pipeline.process_video_async(
                video_path=cfg.io.video,
                detector_config=detector_config,
                output_path=cfg.io.output,
                visualization_config=vis_config
            )
            
            # é€²æ—è¡¨ç¤º
            display_progress(result, cfg.system.progress_interval)
            final_result = result.get_result()
            
        else:
            # åŒæœŸå‡¦ç†
            logging.info("åŒæœŸå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­...")
            final_result = pipeline.process_video(
                video_path=cfg.io.video,
                detector_config=detector_config,
                output_path=cfg.io.output,
                visualization_config=vis_config
            )
        
        processing_time = time.time() - start_time
        
        # çµæœè¡¨ç¤º
        logging.info("ğŸ¯ å‡¦ç†å®Œäº†!")
        logging.info(f"å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
        logging.info(f"å¹³å‡FPS: {final_result.get('average_fps', 0):.2f}")
        logging.info(f"ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {final_result.get('total_frames', 0)}")
        logging.info(f"æ¤œå‡ºãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {sum(1 for det_list in final_result.get('detections', {}).values() if det_list)}")
        
        # çµ±è¨ˆæƒ…å ±ä¿å­˜
        if cfg.io.stats_output:
            stats = calculate_statistics(final_result)
            save_statistics(stats, cfg.io.stats_output)
        
        logging.info(f"å‡ºåŠ›å‹•ç”»: {cfg.io.output}")
        
    except Exception as e:
        logging.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main() 