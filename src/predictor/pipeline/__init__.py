"""
Parallel Processing Pipeline Module
===================================

ğŸš€ é«˜åŠ¹ç‡ãƒ†ãƒ‹ã‚¹ãƒœãƒ¼ãƒ«æ¤œå‡ºãƒ»å¯è¦–åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜åŠ¹ç‡ãªãƒœãƒ¼ãƒ«æ¤œå‡ºãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æä¾›ã—ã¾ã™ã€‚
GPUåˆ©ç”¨ç‡ã®æœ€å¤§åŒ–ã¨å¾…æ©Ÿæ™‚é–“ã®æœ€å°åŒ–ã‚’ç›®çš„ã¨ã—ãŸè¨­è¨ˆã«ã‚ˆã‚Šã€
ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ã¨å¤§å®¹é‡å‹•ç”»å‡¦ç†ã®ä¸¡æ–¹ã«å¯¾å¿œã—ã¾ã™ã€‚

ä¸»ãªç‰¹å¾´:
- ğŸ”„ ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ä¸¦åˆ—å‡¦ç†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- âš¡ GPUå¾…æ©Ÿæ™‚é–“æœ€å°åŒ–è¨­è¨ˆ
- ğŸ“Š åŠ¹ç‡çš„ãªã‚­ãƒ¥ãƒ¼ãƒ™ãƒ¼ã‚¹å‡¦ç†
- ğŸ¥ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ»ãƒãƒƒãƒå‡¦ç†çµ±åˆå¯¾å¿œ

========================================
ğŸ—ï¸ Pipeline Architecture
========================================

Thread Distribution:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frame Reader   â”‚ -> â”‚  Preprocessor    â”‚ -> â”‚ Inference Queue â”‚
â”‚     Thread      â”‚    â”‚     Thread       â”‚    â”‚   (GPU Ready)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚ Visualization   â”‚ <- â”‚  Postprocessor   â”‚ <----------â”˜
â”‚     Thread      â”‚    â”‚     Thread       â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ <- â”‚  GPU Inference  â”‚
                                                â”‚     Thread      â”‚
                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Queue-Based Data Flow:
- Raw Frame Queue: ãƒ•ãƒ¬ãƒ¼ãƒ èª­ã¿è¾¼ã¿ â†’ å‰å‡¦ç†
- Tensor Queue: å‰å‡¦ç†æ¸ˆã¿ â†’ GPUæ¨è«–ï¼ˆãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ï¼‰
- Result Queue: æ¨è«–çµæœ â†’ å¾Œå‡¦ç†
- Render Queue: æ¤œå‡ºçµæœ â†’ å¯è¦–åŒ–å‡ºåŠ›

========================================
ğŸš€ Quick Start
========================================

```python
from src.predictor.pipeline import VideoPipeline, PipelineConfig

# 1. åŸºæœ¬çš„ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
pipeline = VideoPipeline()
output_path = pipeline.process_video(
    video_path="input.mp4",
    detector_config={"model_type": "wasb_sbdt", "model_path": "model.pth"},
    output_path="output.mp4"
)

# 2. ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã«ã‚ˆã‚‹æœ€é©åŒ–
config = PipelineConfig(
    frame_buffer_size=50,      # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º
    tensor_buffer_size=20,     # ãƒ†ãƒ³ã‚½ãƒ«ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º
    num_preprocessing_threads=2,  # å‰å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰æ•°
    gpu_batch_size=4,          # GPU ãƒãƒƒãƒã‚µã‚¤ã‚º
    enable_visualization=True   # å¯è¦–åŒ–æœ‰åŠ¹
)

pipeline = VideoPipeline(config)
pipeline.process_video_async(video_path, detector_config, output_path)
```

========================================
ğŸ“Š Performance Features
========================================

GPU Utilization Optimization:
- Tensor Pre-buffering: å‰å‡¦ç†æ¸ˆã¿ãƒ†ãƒ³ã‚½ãƒ«ã®äº‹å‰è“„ç©
- Batch Inference: è¤‡æ•°ãƒ•ãƒ¬ãƒ¼ãƒ ã®åŒæ™‚æ¨è«–
- Pipeline Parallelism: GPUå¾…æ©Ÿæ™‚é–“ã®å®Œå…¨æ’é™¤

Memory Management:
- Circular Buffer: å›ºå®šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
- Streaming Processing: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå¤§å®¹é‡å‹•ç”»å‡¦ç†
- Resource Cleanup: è‡ªå‹•ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†

Thread Safety:
- Lock-free Queues: é«˜é€Ÿãƒ‡ãƒ¼ã‚¿äº¤æ›
- Exception Handling: ã‚¹ãƒ¬ãƒƒãƒ‰é–“ã‚¨ãƒ©ãƒ¼ä¼æ’­
- Graceful Shutdown: å®‰å…¨ãªå‡¦ç†çµ‚äº†
"""

from .video_pipeline import VideoPipeline
from .config import PipelineConfig
from .async_processor import AsyncVideoProcessor
from . import examples

__all__ = [
    'VideoPipeline',
    'PipelineConfig', 
    'AsyncVideoProcessor',
    'examples'
] 