"""
Ball Patch Dataset
æ—¢å­˜ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«åˆã‚ã›ãŸ16x16ãƒ‘ãƒƒãƒ2å€¤åˆ†é¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
"""

import os
import json
import random
import cv2
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import albumentations as A
from albumentations.pytorch import ToTensorV2
from typing import List, Tuple, Dict, Optional, Union
from pathlib import Path
import logging
from collections import defaultdict
from tqdm import tqdm

logger = logging.getLogger(__name__)


class BallPatchDataset(Dataset):
    """
    16x16ãƒ‘ãƒƒãƒã§ã®ãƒœãƒ¼ãƒ«2å€¤åˆ†é¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    
    æ—¢å­˜ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å½¢å¼ã«å¯¾å¿œ:
    - game_id/clip_id æ§‹é€ 
    - original_path ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    - keypoints [x, y, visibility] å½¢å¼
    """
    
    def __init__(self,
                 annotation_file: str,
                 images_dir: str,
                 patch_size: int = 16,
                 negative_ratio: float = 2.0,
                 min_ball_visibility: float = 0.5,
                 transform: Optional[A.Compose] = None,
                 split: str = "train",
                 train_ratio: float = 0.8,
                 val_ratio: float = 0.1,
                 seed: int = 42,
                 position_noise: int = 2,  # Â±ãƒ”ã‚¯ã‚»ãƒ«æ•°ã®ä½ç½®ãƒã‚¤ã‚º
                 cache_images: bool = False):
        """
        Args:
            annotation_file (str): ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå½¢å¼ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«
            images_dir (str): ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (datasets/ball/images)
            patch_size (int): ãƒ‘ãƒƒãƒã‚µã‚¤ã‚º
            negative_ratio (float): è² ä¾‹ã®å‰²åˆï¼ˆæ­£ä¾‹ã®ä½•å€ã‹ï¼‰
            min_ball_visibility (float): æœ€å°å¯è¦–æ€§é–¾å€¤
            transform (Optional): ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
            split (str): "train", "val", "test"
            train_ratio (float): å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å‰²åˆ
            val_ratio (float): æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿å‰²åˆ
            seed (int): ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã‚·ãƒ¼ãƒ‰
            position_noise (int): ãƒœãƒ¼ãƒ«ä½ç½®ã®Â±ãƒ”ã‚¯ã‚»ãƒ«ãƒã‚¤ã‚ºï¼ˆæ¤œå‡ºèª¤å·®å†ç¾ï¼‰
            cache_images (bool): ç”»åƒã‚’ãƒ¡ãƒ¢ãƒªã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ã‹
        """
        self.annotation_file = annotation_file
        self.images_dir = Path(images_dir)
        self.patch_size = patch_size
        self.negative_ratio = negative_ratio
        self.min_ball_visibility = min_ball_visibility
        self.cache_images = cache_images
        self.split = split
        self.position_noise = position_noise
        
        # Load annotations
        self.data = self._load_annotations()
        
        # Split data by clip (to avoid temporal leakage)
        self.split_data = self._split_data_by_clip(train_ratio, val_ratio, seed)
        
        # Generate patch samples
        self.samples = self._generate_samples()
        
        # Set up transforms
        if transform is None:
            self.transform = self._get_default_transforms()
        else:
            self.transform = transform
            
        # Image cache
        self.image_cache = {} if cache_images else None
        
        logger.info(f"Dataset initialized [{split}]: {len(self.samples)} samples "
                   f"({self._count_positive()} positive, {self._count_negative()} negative)")
        logger.info(f"Position noise: Â±{self.position_noise}px (simulating detection uncertainty)")
        
    def _load_annotations(self) -> Dict:
        """ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®èª­ã¿è¾¼ã¿"""
        with open(self.annotation_file, 'r') as f:
            data = json.load(f)
            
        # Create mappings
        self.images_by_id = {img['id']: img for img in data['images']}
        
        # Group annotations by image (only ball category)
        self.annotations_by_image = {}
        for ann in data['annotations']:
            if ann.get('category_id') == 1:  # Ball category
                image_id = ann['image_id']
                self.annotations_by_image[image_id] = ann
                
        logger.info(f"Loaded {len(self.images_by_id)} images, {len(self.annotations_by_image)} ball annotations")
        return data
        
    def _split_data_by_clip(self, train_ratio: float, val_ratio: float, seed: int) -> List[int]:
        """ã‚¯ãƒªãƒƒãƒ—å˜ä½ã§ã®ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆæ™‚é–“çš„ãƒªãƒ¼ã‚¯ã‚’é˜²ããŸã‚ï¼‰"""
        # Group by (game_id, clip_id)
        clip_groups = defaultdict(list)
        for img_id, img_info in self.images_by_id.items():
            key = (img_info['game_id'], img_info['clip_id'])
            clip_groups[key].append(img_id)
            
        # Split clips
        clip_keys = list(clip_groups.keys())
        random.Random(seed).shuffle(clip_keys)
        
        n_clips = len(clip_keys)
        n_train = int(n_clips * train_ratio)
        n_val = int(n_clips * val_ratio)
        
        split_map = {
            "train": clip_keys[:n_train],
            "val": clip_keys[n_train:n_train + n_val],
            "test": clip_keys[n_train + n_val:],
        }
        
        # Get image IDs for this split
        target_clips = split_map[self.split]
        split_image_ids = []
        for clip_key in target_clips:
            split_image_ids.extend(clip_groups[clip_key])
            
        logger.info(f"Split [{self.split}]: {len(target_clips)} clips, {len(split_image_ids)} images")
        print(f"ğŸ“Š {self.split}ãƒ‡ãƒ¼ã‚¿: {len(target_clips)}ã‚¯ãƒªãƒƒãƒ—, {len(split_image_ids)}ç”»åƒ")
        return split_image_ids
        
    def _generate_samples(self) -> List[Dict]:
        """æ­£ä¾‹ãƒ»è² ä¾‹ã‚µãƒ³ãƒ—ãƒ«ã®ç”Ÿæˆï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰"""
        samples = []
        
        print(f"ğŸ”„ {self.split} split: ãƒ‘ãƒƒãƒç”Ÿæˆä¸­...")
        
        # Step 1: ç”»åƒå¯¸æ³•ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ§‹ç¯‰
        print("ğŸ“ ç”»åƒå¯¸æ³•ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ§‹ç¯‰ä¸­...")
        image_dimensions = {}
        
        with tqdm(self.split_data, desc="å¯¸æ³•å–å¾—", unit="images", ncols=100) as pbar:
            for image_id in pbar:
                img_info = self.images_by_id[image_id]
                image_path = self.images_dir / img_info['original_path']
                
                if image_path.exists():
                    try:
                        img = cv2.imread(str(image_path))
                        if img is not None:
                            h, w = img.shape[:2]
                            image_dimensions[image_id] = (w, h, str(image_path))
                    except Exception as e:
                        logger.warning(f"Failed to load image {image_path}: {e}")
                        continue
                        
                pbar.set_postfix({'ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿': len(image_dimensions)})
        
        print(f"âœ… ç”»åƒå¯¸æ³•ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Œäº†: {len(image_dimensions)}ç”»åƒ")
        
        # Step 2: æ­£ä¾‹ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆï¼ˆé«˜é€ŸåŒ–æ¸ˆã¿ï¼‰
        positive_count = 0
        print("ğŸ“ æ­£ä¾‹ãƒ‘ãƒƒãƒç”Ÿæˆä¸­...")
        
        with tqdm(self.split_data, desc="æ­£ä¾‹ç”Ÿæˆ", unit="images", ncols=100) as pbar:
            for image_id in pbar:
                if image_id not in self.annotations_by_image or image_id not in image_dimensions:
                    continue
                    
                ann = self.annotations_by_image[image_id]
                w, h, image_path = image_dimensions[image_id]
                
                # Check visibility
                keypoints = ann['keypoints']
                if len(keypoints) >= 3 and keypoints[2] >= self.min_ball_visibility:
                    # Get original ball position
                    orig_x, orig_y = int(keypoints[0]), int(keypoints[1])
                    
                    # Add realistic position noise to simulate detection uncertainty
                    noise_x = random.randint(-self.position_noise, self.position_noise)
                    noise_y = random.randint(-self.position_noise, self.position_noise)
                    x = orig_x + noise_x
                    y = orig_y + noise_y
                    
                    # Check if patch fits in image bounds (with noise)
                    half_patch = self.patch_size // 2
                    
                    if half_patch <= x < w - half_patch and half_patch <= y < h - half_patch:
                        samples.append({
                            'image_path': image_path,
                            'center_x': x,
                            'center_y': y,
                            'label': 1,  # Positive
                            'image_id': image_id,
                            'visibility': keypoints[2]
                        })
                        positive_count += 1
                        
                pbar.set_postfix({'æ­£ä¾‹æ•°': positive_count})
                    
        print(f"âœ… æ­£ä¾‹ç”Ÿæˆå®Œäº†: {positive_count}ã‚µãƒ³ãƒ—ãƒ«")
        
        # Step 3: è² ä¾‹ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆï¼ˆå¤§å¹…é«˜é€ŸåŒ–ï¼‰
        negative_count = int(positive_count * self.negative_ratio)
        negative_generated = 0
        
        print(f"ğŸ“ è² ä¾‹ãƒ‘ãƒƒãƒç”Ÿæˆä¸­ (ç›®æ¨™: {negative_count}ã‚µãƒ³ãƒ—ãƒ«)...")
        
        # æ­£ä¾‹ä½ç½®ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
        positive_positions_by_image = {}
        for sample in samples:
            if sample['label'] == 1:
                image_id = sample['image_id']
                if image_id not in positive_positions_by_image:
                    positive_positions_by_image[image_id] = []
                positive_positions_by_image[image_id].append((sample['center_x'], sample['center_y']))
        
        # åŠ¹ç‡çš„ãªè² ä¾‹ç”Ÿæˆ
        available_images = [img_id for img_id in image_dimensions.keys()]
        random.shuffle(available_images)
        
        with tqdm(total=negative_count, desc="è² ä¾‹ç”Ÿæˆ", unit="samples", ncols=100) as pbar:
            for image_id in available_images:
                if negative_generated >= negative_count:
                    break
                    
                w, h, image_path = image_dimensions[image_id]
                positive_positions = positive_positions_by_image.get(image_id, [])
                
                # å„ç”»åƒã‹ã‚‰è¤‡æ•°ã®è² ä¾‹ã‚’åŠ¹ç‡çš„ã«ç”Ÿæˆ
                samples_per_image = min(8, negative_count - negative_generated)  # å¢—é‡
                batch_samples = self._generate_negative_batch(
                    image_path, image_id, w, h, positive_positions, samples_per_image
                )
                
                samples.extend(batch_samples)
                negative_generated += len(batch_samples)
                
                pbar.update(len(batch_samples))
                pbar.set_postfix({'æ®‹ã‚Š': negative_count - negative_generated})
                    
        print(f"âœ… è² ä¾‹ç”Ÿæˆå®Œäº†: {negative_generated}ã‚µãƒ³ãƒ—ãƒ«")
        print(f"ğŸ¯ ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {positive_count + negative_generated} (æ­£ä¾‹: {positive_count}, è² ä¾‹: {negative_generated})")
        
        logger.info(f"Generated {positive_count} positive, {negative_generated} negative samples")
        return samples
    
    def _generate_negative_batch(self, image_path: str, image_id: int, width: int, height: int, 
                               positive_positions: List[Tuple[int, int]], num_samples: int) -> List[Dict]:
        """åŠ¹ç‡çš„ãªè² ä¾‹ãƒãƒƒãƒç”Ÿæˆ"""
        batch_samples = []
        half_patch = self.patch_size // 2
        min_distance = self.patch_size
        
        # æœ‰åŠ¹ãªåº§æ¨™ç¯„å›²
        x_min, x_max = half_patch, width - half_patch - 1
        y_min, y_max = half_patch, height - half_patch - 1
        
        if x_min >= x_max or y_min >= y_max:
            return []  # ç”»åƒãŒå°ã•ã™ãã‚‹
        
        # ã‚°ãƒªãƒƒãƒ‰ãƒ™ãƒ¼ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆåŠ¹ç‡çš„ï¼‰
        attempts = 0
        max_attempts = num_samples * 20  # åˆ¶é™ã‚’å¢—ã‚„ã™
        
        while len(batch_samples) < num_samples and attempts < max_attempts:
            # ãƒ©ãƒ³ãƒ€ãƒ ä½ç½®ç”Ÿæˆ
            x = random.randint(x_min, x_max)
            y = random.randint(y_min, y_max)
            
            # æ­£ä¾‹ä½ç½®ã¨ã®è·é›¢ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
            too_close = False
            for pos_x, pos_y in positive_positions:
                if abs(x - pos_x) < min_distance and abs(y - pos_y) < min_distance:
                    # ç°¡æ˜“è·é›¢ãƒã‚§ãƒƒã‚¯ï¼ˆé«˜é€Ÿï¼‰
                    too_close = True
                    break
                    
            if not too_close:
                batch_samples.append({
                    'image_path': image_path,
                    'center_x': x,
                    'center_y': y,
                    'label': 0,  # Negative
                    'image_id': image_id,
                    'visibility': 0.0
                })
                
            attempts += 1
            
        return batch_samples
        
    def _get_default_transforms(self) -> A.Compose:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ"""
        if self.split == "train":
            # Training augmentations
            return A.Compose([
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.3),
                A.Rotate(limit=15, p=0.5),
                A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),
                A.GaussNoise(var_limit=10.0, p=0.3),  
                A.Blur(blur_limit=3, p=0.2),
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                ToTensorV2()
            ])
        else:
            # Validation/test - minimal augmentation
            return A.Compose([
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                ToTensorV2()
            ])
        
    def _load_image(self, image_path: str) -> np.ndarray:
        """ç”»åƒã®èª­ã¿è¾¼ã¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰"""
        if self.image_cache is not None:
            if image_path not in self.image_cache:
                img = cv2.imread(image_path)
                if img is None:
                    raise FileNotFoundError(f"Image not found: {image_path}")
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                self.image_cache[image_path] = img
            return self.image_cache[image_path]
        else:
            img = cv2.imread(image_path)
            if img is None:
                raise FileNotFoundError(f"Image not found: {image_path}")
            return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            
    def _count_positive(self) -> int:
        """æ­£ä¾‹æ•°ã®ã‚«ã‚¦ãƒ³ãƒˆ"""
        return sum(1 for sample in self.samples if sample['label'] == 1)
        
    def _count_negative(self) -> int:
        """è² ä¾‹æ•°ã®ã‚«ã‚¦ãƒ³ãƒˆ"""
        return sum(1 for sample in self.samples if sample['label'] == 0)
        
    def __len__(self) -> int:
        return len(self.samples)
        
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        sample = self.samples[idx]
        
        try:
            # Load image
            img = self._load_image(sample['image_path'])
            
            # Extract patch
            half_patch = self.patch_size // 2
            center_x, center_y = sample['center_x'], sample['center_y']
            
            x1 = center_x - half_patch
            y1 = center_y - half_patch
            x2 = x1 + self.patch_size
            y2 = y1 + self.patch_size
            
            # Handle boundary cases
            h, w = img.shape[:2]
            if x1 < 0 or y1 < 0 or x2 > w or y2 > h:
                # Pad image if necessary
                pad_left = max(0, -x1)
                pad_top = max(0, -y1)
                pad_right = max(0, x2 - w)
                pad_bottom = max(0, y2 - h)
                
                img = np.pad(img, ((pad_top, pad_bottom), (pad_left, pad_right), (0, 0)), 
                           mode='reflect')
                
                # Adjust coordinates
                x1 += pad_left
                y1 += pad_top
                x2 = x1 + self.patch_size
                y2 = y1 + self.patch_size
                
            patch = img[y1:y2, x1:x2]
            
            # Ensure patch size
            if patch.shape[:2] != (self.patch_size, self.patch_size):
                patch = cv2.resize(patch, (self.patch_size, self.patch_size))
                
            # Apply transforms
            if self.transform:
                transformed = self.transform(image=patch)
                patch = transformed['image']
                
            label = torch.tensor(sample['label'], dtype=torch.float32)
            
            return patch, label
            
        except Exception as e:
            logger.warning(f"Error loading sample {idx}: {e}")
            # Return a zero patch with negative label as fallback
            zero_patch = torch.zeros(3, self.patch_size, self.patch_size)
            return zero_patch, torch.tensor(0.0, dtype=torch.float32)


def create_dataloaders(annotation_file: str,
                      images_dir: str,
                      batch_size: int = 32,
                      num_workers: int = 4,
                      patch_size: int = 16,
                      negative_ratio: float = 2.0,
                      min_ball_visibility: float = 0.5,
                      train_ratio: float = 0.8,
                      val_ratio: float = 0.1,
                      seed: int = 42,
                      position_noise: int = 2) -> Tuple[DataLoader, DataLoader]:
    """
    å­¦ç¿’ãƒ»æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ
    
    Args:
        annotation_file (str): ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«
        images_dir (str): ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        batch_size (int): ãƒãƒƒãƒã‚µã‚¤ã‚º
        num_workers (int): ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
        patch_size (int): ãƒ‘ãƒƒãƒã‚µã‚¤ã‚º
        negative_ratio (float): è² ä¾‹ã®å‰²åˆ
        min_ball_visibility (float): æœ€å°å¯è¦–æ€§
        train_ratio (float): å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å‰²åˆ
        val_ratio (float): æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿å‰²åˆ
        seed (int): ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã‚·ãƒ¼ãƒ‰
        position_noise (int): ãƒœãƒ¼ãƒ«ä½ç½®ãƒã‚¤ã‚ºï¼ˆÂ±ãƒ”ã‚¯ã‚»ãƒ«ï¼‰
        
    Returns:
        Tuple[DataLoader, DataLoader]: å­¦ç¿’ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    """
    # Create datasets
    train_dataset = BallPatchDataset(
        annotation_file=annotation_file,
        images_dir=images_dir,
        patch_size=patch_size,
        negative_ratio=negative_ratio,
        min_ball_visibility=min_ball_visibility,
        split="train",
        train_ratio=train_ratio,
        val_ratio=val_ratio,
        seed=seed,
        position_noise=position_noise
    )
    
    val_dataset = BallPatchDataset(
        annotation_file=annotation_file,
        images_dir=images_dir,
        patch_size=patch_size,
        negative_ratio=negative_ratio,
        min_ball_visibility=min_ball_visibility,
        split="val",
        train_ratio=train_ratio,
        val_ratio=val_ratio,
        seed=seed,
        position_noise=0  # æ¤œè¨¼æ™‚ã¯ãƒã‚¤ã‚ºãªã—
    )
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True  # Drop incomplete batches
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    return train_loader, val_loader


# Test function
if __name__ == "__main__":
    # Test dataset loading
    annotation_file = "datasets/ball/coco_annotations_ball_pose_court.json"
    images_dir = "datasets/ball/images"
    
    dataset = BallPatchDataset(
        annotation_file=annotation_file,
        images_dir=images_dir,
        patch_size=16,
        split="train"
    )
    
    print(f"Dataset size: {len(dataset)}")
    
    # Test a sample
    if len(dataset) > 0:
        patch, label = dataset[0]
        print(f"Patch shape: {patch.shape}")
        print(f"Label: {label}")
        print("Dataset loading successful!")
    else:
        print("No samples found in dataset!") 