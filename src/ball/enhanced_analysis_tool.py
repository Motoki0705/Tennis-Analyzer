"""
Enhanced Ball Analysis Tool with 3-Stage Filtering
3æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ çµ±åˆåˆ†æãƒ„ãƒ¼ãƒ«

Stage 1: ball_tracker confidence filtering
Stage 2: Local classifier (16x16 patch)  
Stage 3: Trajectory consistency validation
"""

import os
import sys
import cv2
import numpy as np
import torch
import json
from typing import List, Dict, Tuple, Optional, Union
from pathlib import Path
import logging
from dataclasses import dataclass, asdict
import time

# Add third_party paths
project_root = Path(__file__).parent.parent.parent
third_party_path = project_root / "third_party" / "WASB-SBDT" / "src"
sys.path.insert(0, str(third_party_path))

# ball_tracker imports
try:
    from models import ballseg, deepball
    from dataloaders import dataset_loader
    from detectors import detector, postprocessor
    from utils import utils
    BALL_TRACKER_AVAILABLE = True
    print(f"âœ… ball_tracker modules loaded from: {third_party_path}")
except ImportError as e:
    BALL_TRACKER_AVAILABLE = False
    print(f"âš ï¸ ball_tracker not available: {e}")
    print(f"   Search path: {third_party_path}")
    print("   Available modules will be checked during runtime")

# Local classifier imports
from .local_classifier import LocalClassifierInference, BallDetection, ThreeStageFilter

logging.basicConfig(level=logging.ERROR)
logger = logging.getLogger(__name__)


@dataclass
class AnalysisResult:
    """åˆ†æçµæœã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    frame_idx: int
    detections: List[BallDetection]
    stage1_count: int = 0
    stage2_count: int = 0
    stage3_count: int = 0
    processing_time: float = 0.0


class EnhancedBallAnalyzer:
    """
    3æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµ±åˆãƒœãƒ¼ãƒ«åˆ†æå™¨
    """
    
    def __init__(self,
                 ball_tracker_config: str,
                 ball_tracker_weights: str,
                 local_classifier_model: str,
                 local_classifier_type: str = "standard",
                 stage1_threshold: float = 0.5,
                 stage2_threshold: float = 0.5,
                 stage3_max_distance: float = 50.0,
                 device: str = "cuda"):
        """
        Args:
            ball_tracker_config (str): ball_trackerè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
            ball_tracker_weights (str): ball_trackeré‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«
            local_classifier_model (str): ãƒ­ãƒ¼ã‚«ãƒ«åˆ†é¡å™¨ãƒ¢ãƒ‡ãƒ«
            local_classifier_type (str): ãƒ­ãƒ¼ã‚«ãƒ«åˆ†é¡å™¨ã‚¿ã‚¤ãƒ—
            stage1_threshold (float): Stage1é–¾å€¤
            stage2_threshold (float): Stage2é–¾å€¤
            stage3_max_distance (float): Stage3æœ€å¤§è·é›¢
            device (str): ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹
        """
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        
        # Stage 1: ball_tracker (try to load regardless of BALL_TRACKER_AVAILABLE)
        print(f"ğŸ”§ BALL_TRACKER_AVAILABLE: {BALL_TRACKER_AVAILABLE}")
        print("ğŸ”„ Attempting to load ball_tracker using video_demo pattern...")
        self.ball_tracker = self._load_ball_tracker(ball_tracker_config, ball_tracker_weights)
        print(f"ğŸ” ball_tracker loaded result: {self.ball_tracker is not None}")
        
        if self.ball_tracker is None:
            print("âš ï¸ ball_tracker not available, will use demo grid sampling")
            logger.warning("ball_tracker not available, using dummy detector")
        else:
            # Update frames needed based on loaded ball_tracker
            self.frames_needed = getattr(self.ball_tracker, 'frames_in', 1)
            print(f"ğŸ”§ Updated frames_needed: {self.frames_needed}")
            
        # Stage 2: Local classifier
        self.local_classifier = LocalClassifierInference(
            model_path=local_classifier_model,
            model_type=local_classifier_type,
            confidence_threshold=stage2_threshold,
            device=device
        )
        
        # Stage 3: Three-stage filter
        self.three_stage_filter = ThreeStageFilter(
            ball_tracker_model=self.ball_tracker,
            local_classifier=self.local_classifier,
            stage1_threshold=stage1_threshold,
            stage3_max_distance=stage3_max_distance
        )
        
        # Configuration
        self.stage1_threshold = stage1_threshold
        self.stage2_threshold = stage2_threshold
        self.stage3_max_distance = stage3_max_distance
        
        # Frame buffer for WASB-SBDT (will be set after ball_tracker loading)
        self.frame_buffer = []
        self.frames_needed = 1  # Default, will be updated after ball_tracker loading
        
        logger.info("Enhanced Ball Analyzer initialized")
        logger.info(f"  Device: {self.device}")
        logger.info(f"  Stage1 threshold: {stage1_threshold}")
        logger.info(f"  Stage2 threshold: {stage2_threshold}")
        logger.info(f"  Stage3 max distance: {stage3_max_distance}")
        logger.info(f"  Frames needed: {self.frames_needed}")
        
    def _load_ball_tracker(self, config_path: str, weights_path: str):
        """ball_trackerãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆWASB-SBDT video_demoæ–¹å¼ï¼‰"""
        try:
            print("ğŸ”§ Loading ball_tracker using video_demo pattern...")
            
            # Import WASB-SBDT modules directly
            import sys
            from pathlib import Path
            
            # Add WASB-SBDT to path
            wasb_path = Path(__file__).parent.parent.parent / "third_party" / "WASB-SBDT" / "src"
            sys.path.insert(0, str(wasb_path))
            
            from omegaconf import OmegaConf
            
            # Load simple config (avoiding Hydra)
            cfg = self._load_simple_config()
            cfg.detector.model_path = weights_path
            
            # Import WASB-SBDT modules
            from models import build_model
            from utils.image import get_affine_transform
            from detectors.postprocessor import TracknetV2Postprocessor
            import dataloaders.img_transforms as T
            
            # Create SimpleDetector following video_demo pattern
            from .wasb_simple_detector import WASBSimpleDetector
            
            detector = WASBSimpleDetector(cfg, self.device)
            
            print(f"âœ… ball_tracker loaded successfully: frames_in={detector.frames_in}")
            logger.info(f"ball_tracker loaded: {config_path}")
            return detector
            
        except Exception as e:
            print(f"âŒ Failed to load ball_tracker: {e}")
            logger.warning(f"Failed to load ball_tracker: {e}")
            logger.info("Continuing without ball_tracker (Stage 1 will be disabled)")
            return None
            
    def _load_simple_config(self):
        """Load WASB-SBDT config without Hydra (from video_demo)"""
        from omegaconf import OmegaConf
        
        cfg = {
            'model': {
                'name': 'hrnet',
                'frames_in': 3,
                'frames_out': 3,
                'inp_height': 288,
                'inp_width': 512,
                'out_height': 288, 
                'out_width': 512,
                'rgb_diff': False,
                'out_scales': [0],
                'MODEL': {
                    'EXTRA': {
                        'FINAL_CONV_KERNEL': 1,
                        'PRETRAINED_LAYERS': ['*'],
                        'STEM': {'INPLANES': 64, 'STRIDES': [1,1]},
                        'STAGE1': {
                            'NUM_MODULES': 1, 'NUM_BRANCHES': 1, 'BLOCK': 'BOTTLENECK',
                            'NUM_BLOCKS': [1], 'NUM_CHANNELS': [32], 'FUSE_METHOD': 'SUM'
                        },
                        'STAGE2': {
                            'NUM_MODULES': 1, 'NUM_BRANCHES': 2, 'BLOCK': 'BASIC',
                            'NUM_BLOCKS': [2,2], 'NUM_CHANNELS': [16,32], 'FUSE_METHOD': 'SUM'
                        },
                        'STAGE3': {
                            'NUM_MODULES': 1, 'NUM_BRANCHES': 3, 'BLOCK': 'BASIC', 
                            'NUM_BLOCKS': [2,2,2], 'NUM_CHANNELS': [16,32,64], 'FUSE_METHOD': 'SUM'
                        },
                        'STAGE4': {
                            'NUM_MODULES': 1, 'NUM_BRANCHES': 4, 'BLOCK': 'BASIC',
                            'NUM_BLOCKS': [2,2,2,2], 'NUM_CHANNELS': [16,32,64,128], 'FUSE_METHOD': 'SUM'
                        },
                        'DECONV': {'NUM_DECONVS': 0, 'KERNEL_SIZE': [], 'NUM_BASIC_BLOCKS': 2}
                    },
                    'INIT_WEIGHTS': True
                }
            },
            'detector': {
                'model_path': None,
                'postprocessor': {
                    'name': 'tracknetv2',
                    'score_threshold': 0.5,
                    'scales': [0],
                    'blob_det_method': 'concomp',
                    'use_hm_weight': True
                }
            },
            'tracker': {
                'name': 'online',
                'max_disp': 100
            },
            'dataloader': {
                'heatmap': {
                    'sigmas': {0: 2.0}
                }
            }
        }
        return OmegaConf.create(cfg)
            
    def detect_stage1(self, frame: np.ndarray) -> List[BallDetection]:
        """Stage 1: ball_trackeræ¤œå‡º"""
        print(f"ğŸ” Stage1 detect_stage1 called: frame_shape={frame.shape}")
        print(f"ğŸ” ball_tracker is None: {self.ball_tracker is None}")
        print(f"ğŸ” stage1_threshold: {self.stage1_threshold}")
        
        if self.ball_tracker is None:
            # ball_trackerãŒåˆ©ç”¨ã§ããªã„å ´åˆã€ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ä»£æ›¿
            print("ğŸ”„ ball_tracker not available, using grid sampling for demonstration")
            logger.info("ball_tracker not available, using grid sampling for demonstration")
            detections = self._generate_demo_detections(frame)
            print(f"ğŸ” Demo detections generated: {len(detections)}")
            return detections
            
        try:
            # Convert RGB to BGR for WASB-SBDT
            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            
            # Add frame to buffer
            self.frame_buffer.append(frame_bgr)
            
            # Keep only the required number of frames
            if len(self.frame_buffer) > self.frames_needed:
                self.frame_buffer.pop(0)
                
            # Need enough frames for detection
            if len(self.frame_buffer) < self.frames_needed:
                print(f"ğŸ”„ Building frame buffer: {len(self.frame_buffer)}/{self.frames_needed}")
                return []
                
            print(f"ğŸ”„ Using WASB-SBDT inference with {len(self.frame_buffer)} frames")
            
            # WASB-SBDT detection
            wasb_detections = self.ball_tracker.process_frames(self.frame_buffer)
            
            # Convert to BallDetection format
            detections = []
            for det in wasb_detections:
                xy = det['xy']
                score = det['score']
                
                if score >= self.stage1_threshold:
                    ball_det = BallDetection(
                        x=float(xy[0]),
                        y=float(xy[1]),
                        confidence=float(score),
                        stage1_conf=float(score)
                    )
                    detections.append(ball_det)
            
            print(f"ğŸ” WASB-SBDT result: {len(wasb_detections)} raw â†’ {len(detections)} filtered (threshold={self.stage1_threshold})")
            return detections
            
        except Exception as e:
            print(f"âŒ Stage1 detection failed: {e}")
            logger.error(f"Stage1 detection failed: {e}")
            # Fallback to demo detections
            return self._generate_demo_detections(frame)
            
    def _generate_demo_detections(self, frame: np.ndarray) -> List[BallDetection]:
        """ãƒ‡ãƒ¢ç”¨ã®ã‚°ãƒªãƒƒãƒ‰æ¤œå‡ºï¼ˆball_trackerä»£æ›¿ï¼‰"""
        h, w = frame.shape[:2]
        detections = []
        
        print(f"ğŸ¯ _generate_demo_detections called: frame_size={w}x{h}")
        
        # ã‚ˆã‚Šç¾å®Ÿçš„ãªãƒ‡ãƒ¢æ¤œå‡ºï¼ˆball_trackerä»£æ›¿ï¼‰
        # é€šå¸¸ã®ball_trackerã¯1-5æ¤œå‡º/frameãªã®ã§ã€ãã‚Œã«è¿‘ã„æ•°ã«èª¿æ•´
        step = min(200, max(150, min(w, h) // 3))  # ã‚ˆã‚Šå¤§ããªé–“éš”
        print(f"ğŸ” Grid step size: {step}px (reduced for realistic detection count)")
        
        grid_count = 0
        detection_count = 0
        max_detections = 8  # æœ€å¤§æ¤œå‡ºæ•°ã‚’åˆ¶é™
        
        for y in range(step, h - step, step):
            for x in range(step, w - step, step):
                if detection_count >= max_detections:
                    break
                    
                grid_count += 1
                # ãƒ©ãƒ³ãƒ€ãƒ ãªä¿¡é ¼åº¦ã‚’ç”Ÿæˆï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
                confidence = np.random.uniform(0.3, 0.9)
                
                if confidence >= self.stage1_threshold:
                    detection = BallDetection(
                        x=float(x),
                        y=float(y),
                        confidence=confidence,
                        stage1_conf=confidence
                    )
                    detections.append(detection)
                    detection_count += 1
                    print(f"  Demo detection [{detection_count}]: ({x},{y}) conf={confidence:.3f}")
            
            if detection_count >= max_detections:
                break
                    
        print(f"ğŸ¯ Grid sampling result: {grid_count} positions tested, {detection_count} detections generated")
        logger.info(f"Generated {len(detections)} demo detections for Stage 2 testing")
        return detections
            
    def analyze_frame(self, frame: np.ndarray) -> AnalysisResult:
        """
        å˜ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ã®3æ®µéšåˆ†æ
        
        Args:
            frame (np.ndarray): å…¥åŠ›ãƒ•ãƒ¬ãƒ¼ãƒ  [H, W, 3]
            
        Returns:
            AnalysisResult: åˆ†æçµæœ
        """
        start_time = time.time()
        
        # Stage 1: ball_tracker detection
        stage1_detections = self.detect_stage1(frame)
        stage1_count = len(stage1_detections)
        
        if not stage1_detections:
            return AnalysisResult(
                frame_idx=0,
                detections=[],
                stage1_count=0,
                stage2_count=0,
                stage3_count=0,
                processing_time=time.time() - start_time
            )
            
        # Stage 2: Local classifier filtering
        stage2_detections = self.local_classifier.classify_detections(frame, stage1_detections)
        stage2_count = len(stage2_detections)
        
        if not stage2_detections:
            return AnalysisResult(
                frame_idx=0,
                detections=[],
                stage1_count=stage1_count,
                stage2_count=0,
                stage3_count=0,
                processing_time=time.time() - start_time
            )
            
        # Stage 3: Trajectory consistency (handled by ThreeStageFilter)
        # For single frame, we'll skip trajectory validation
        final_detections = stage2_detections
        stage3_count = len(final_detections)
        
        processing_time = time.time() - start_time
        
        return AnalysisResult(
            frame_idx=0,
            detections=final_detections,
            stage1_count=stage1_count,
            stage2_count=stage2_count,
            stage3_count=stage3_count,
            processing_time=processing_time
        )
        
    def analyze_video(self, 
                     video_path: str,
                     output_dir: str = None,
                     save_results: bool = True,
                     visualize: bool = True) -> List[AnalysisResult]:
        """
        å‹•ç”»ã®3æ®µéšåˆ†æ
        
        Args:
            video_path (str): å…¥åŠ›å‹•ç”»ãƒ‘ã‚¹
            output_dir (str): å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            save_results (bool): çµæœä¿å­˜ãƒ•ãƒ©ã‚°
            visualize (bool): å¯è¦–åŒ–ãƒ•ãƒ©ã‚°
            
        Returns:
            List[AnalysisResult]: ãƒ•ãƒ¬ãƒ¼ãƒ æ¯ã®åˆ†æçµæœ
        """
        video_path = Path(video_path)
        if not video_path.exists():
            raise FileNotFoundError(f"Video not found: {video_path}")
            
        if output_dir is None:
            output_dir = video_path.parent / f"{video_path.stem}_enhanced_analysis"
        else:
            output_dir = Path(output_dir)
            
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Open video
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            raise ValueError(f"Cannot open video: {video_path}")
            
        # Video properties
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        logger.info(f"Processing video: {video_path}")
        logger.info(f"  Resolution: {width}x{height}")
        logger.info(f"  FPS: {fps}")
        logger.info(f"  Total frames: {total_frames}")
        
        # Initialize video writer for visualization
        video_writer = None
        if visualize:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            output_video_path = output_dir / f"{video_path.stem}_enhanced_analysis.mp4"
            video_writer = cv2.VideoWriter(str(output_video_path), fourcc, fps, (width, height))
            
        # Process frames
        frame_results = []
        frame_idx = 0
        
        print(f"\n{'='*80}")
        print(f"ğŸ¬ 3æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‹•ç”»åˆ†æé–‹å§‹")
        print(f"{'='*80}")
        print(f"ğŸ“¹ å…¥åŠ›: {video_path.name}")
        print(f"ğŸ“Š ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {total_frames}")
        print(f"ğŸ’¾ å‡ºåŠ›: {output_dir}")
        print(f"{'='*80}")
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                    
                # Convert BGR to RGB for analysis
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # Analyze frame
                result = self.analyze_frame(frame_rgb)
                result.frame_idx = frame_idx
                frame_results.append(result)
                
                # Progress reporting
                if frame_idx % 30 == 0:  # Every 30 frames
                    progress = (frame_idx / total_frames) * 100
                    avg_stage1 = np.mean([r.stage1_count for r in frame_results[-30:]])
                    avg_stage2 = np.mean([r.stage2_count for r in frame_results[-30:]])
                    avg_stage3 = np.mean([r.stage3_count for r in frame_results[-30:]])
                    avg_time = np.mean([r.processing_time for r in frame_results[-30:]])
                    
                    print(f"ğŸ¯ ãƒ•ãƒ¬ãƒ¼ãƒ  {frame_idx:6d}/{total_frames} ({progress:5.1f}%)")
                    print(f"   Stage1â†’2â†’3: {avg_stage1:.1f}â†’{avg_stage2:.1f}â†’{avg_stage3:.1f}")
                    print(f"   å‡¦ç†æ™‚é–“: {avg_time*1000:.1f}ms/frame")
                    
                # Visualization
                if visualize and video_writer is not None:
                    vis_frame = self._visualize_detections(frame, result.detections)
                    video_writer.write(vis_frame)
                    
                frame_idx += 1
                
        finally:
            cap.release()
            if video_writer is not None:
                video_writer.release()
                
        print(f"\nâœ… åˆ†æå®Œäº†!")
        print(f"ğŸ“Š å‡¦ç†ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {len(frame_results)}")
        
        # Save results
        if save_results:
            self._save_analysis_results(frame_results, output_dir, video_path.name)
            
        # Generate summary report
        self._generate_summary_report(frame_results, output_dir)
        
        return frame_results
        
    def _visualize_detections(self, frame: np.ndarray, detections: List[BallDetection]) -> np.ndarray:
        """æ¤œå‡ºçµæœã®å¯è¦–åŒ–"""
        vis_frame = frame.copy()
        
        for detection in detections:
            x, y = int(detection.x), int(detection.y)
            
            # Draw circle
            cv2.circle(vis_frame, (x, y), 8, (0, 255, 0), 2)
            
            # Draw confidence text
            conf_text = f"{detection.confidence:.2f}"
            if detection.stage2_conf is not None:
                conf_text += f"|{detection.stage2_conf:.2f}"
                
            cv2.putText(vis_frame, conf_text, (x + 10, y - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
                       
        return vis_frame
        
    def _save_analysis_results(self, results: List[AnalysisResult], output_dir: Path, video_name: str):
        """åˆ†æçµæœã®ä¿å­˜"""
        # Convert to JSON serializable format
        json_results = []
        for result in results:
            json_result = asdict(result)
            # Convert BallDetection objects to dicts
            json_result['detections'] = [asdict(det) for det in result.detections]
            json_results.append(json_result)
            
        # Save as JSON
        json_path = output_dir / "analysis_results.json"
        with open(json_path, 'w') as f:
            json.dump({
                'video_name': video_name,
                'total_frames': len(results),
                'results': json_results
            }, f, indent=2)
            
        logger.info(f"Analysis results saved: {json_path}")
        
    def _generate_summary_report(self, results: List[AnalysisResult], output_dir: Path):
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        total_frames = len(results)
        
        # Statistics
        total_stage1 = sum(r.stage1_count for r in results)
        total_stage2 = sum(r.stage2_count for r in results)
        total_stage3 = sum(r.stage3_count for r in results)
        
        avg_stage1 = total_stage1 / total_frames if total_frames > 0 else 0
        avg_stage2 = total_stage2 / total_frames if total_frames > 0 else 0
        avg_stage3 = total_stage3 / total_frames if total_frames > 0 else 0
        
        stage1_to_stage2_reduction = (1 - total_stage2/total_stage1) * 100 if total_stage1 > 0 else 0
        stage2_to_stage3_reduction = (1 - total_stage3/total_stage2) * 100 if total_stage2 > 0 else 0
        overall_reduction = (1 - total_stage3/total_stage1) * 100 if total_stage1 > 0 else 0
        
        avg_processing_time = np.mean([r.processing_time for r in results]) * 1000  # ms
        
        # Generate report
        report = f"""
# 3æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

## æ¦‚è¦
- ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {total_frames:,}
- å¹³å‡å‡¦ç†æ™‚é–“: {avg_processing_time:.2f}ms/frame
- æ¨å®šFPS: {1000/avg_processing_time:.1f}

## æ®µéšåˆ¥æ¤œå‡ºçµ±è¨ˆ
| Stage | ç·æ¤œå‡ºæ•° | å¹³å‡æ¤œå‡ºæ•°/frame | å‰Šæ¸›ç‡ |
|-------|----------|------------------|--------|
| Stage 1 (ball_tracker) | {total_stage1:,} | {avg_stage1:.2f} | - |
| Stage 2 (local_classifier) | {total_stage2:,} | {avg_stage2:.2f} | {stage1_to_stage2_reduction:.1f}% |
| Stage 3 (trajectory) | {total_stage3:,} | {avg_stage3:.2f} | {stage2_to_stage3_reduction:.1f}% |

## ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åŠ¹æœ
- **Stage 1â†’2å‰Šæ¸›**: {stage1_to_stage2_reduction:.1f}% (ãƒ­ãƒ¼ã‚«ãƒ«åˆ†é¡å™¨åŠ¹æœ)
- **Stage 2â†’3å‰Šæ¸›**: {stage2_to_stage3_reduction:.1f}% (è»Œè·¡ä¸€è²«æ€§åŠ¹æœ)
- **å…¨ä½“å‰Šæ¸›**: {overall_reduction:.1f}% 

## æ€§èƒ½
- å½é™½æ€§å‰Šæ¸›: {overall_reduction:.1f}%
- æ¨è«–é€Ÿåº¦: {1000/avg_processing_time:.1f} FPS
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡: ãƒ­ãƒ¼ã‚«ãƒ«åˆ†é¡å™¨ä½¿ç”¨ã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªç¯€ç´„

## è¨­å®š
- Stage 1 é–¾å€¤: {self.stage1_threshold}
- Stage 2 é–¾å€¤: {self.stage2_threshold}  
- Stage 3 æœ€å¤§è·é›¢: {self.stage3_max_distance}px
"""
        
        # Save report
        report_path = output_dir / "summary_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
            
        print(f"\nğŸ“‹ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ")
        print(f"{'='*50}")
        print(f"ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {total_frames:,}")
        print(f"Stage 1â†’2â†’3: {avg_stage1:.1f}â†’{avg_stage2:.1f}â†’{avg_stage3:.1f}")
        print(f"å…¨ä½“å‰Šæ¸›ç‡: {overall_reduction:.1f}%")
        print(f"å¹³å‡å‡¦ç†æ™‚é–“: {avg_processing_time:.1f}ms")
        print(f"æ¨å®šFPS: {1000/avg_processing_time:.1f}")
        print(f"{'='*50}")
        print(f"ğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
        
        logger.info(f"Summary report saved: {report_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Enhanced Ball Analysis Tool")
    parser.add_argument("--video", required=True, help="Input video path")
    parser.add_argument("--ball_tracker_config", required=True, help="ball_tracker config file")
    parser.add_argument("--ball_tracker_weights", required=True, help="ball_tracker weights file")
    parser.add_argument("--local_classifier", required=True, help="Local classifier model")
    parser.add_argument("--output_dir", help="Output directory")
    parser.add_argument("--stage1_threshold", type=float, default=0.5, help="Stage 1 threshold")
    parser.add_argument("--stage2_threshold", type=float, default=0.5, help="Stage 2 threshold")
    parser.add_argument("--stage3_max_distance", type=float, default=50.0, help="Stage 3 max distance")
    parser.add_argument("--device", default="cuda", help="Device")
    parser.add_argument("--no_visualize", action="store_true", help="Disable visualization")
    
    args = parser.parse_args()
    
    # Check input files
    for path in [args.video, args.ball_tracker_config, args.ball_tracker_weights, args.local_classifier]:
        if not os.path.exists(path):
            logger.error(f"File not found: {path}")
            return
            
    # Create analyzer
    analyzer = EnhancedBallAnalyzer(
        ball_tracker_config=args.ball_tracker_config,
        ball_tracker_weights=args.ball_tracker_weights,
        local_classifier_model=args.local_classifier,
        stage1_threshold=args.stage1_threshold,
        stage2_threshold=args.stage2_threshold,
        stage3_max_distance=args.stage3_max_distance,
        device=args.device
    )
    
    # Analyze video
    results = analyzer.analyze_video(
        video_path=args.video,
        output_dir=args.output_dir,
        visualize=not args.no_visualize
    )
    
    logger.info("Enhanced analysis completed successfully!")


if __name__ == "__main__":
    main() 