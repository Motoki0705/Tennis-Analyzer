#!/usr/bin/env python3
"""
Tennis Analysis Gradio Application
=================================

ğŸ¾ ãƒ†ãƒ‹ã‚¹å‹•ç”»ãƒ»ç”»åƒç·åˆè§£æã‚·ã‚¹ãƒ†ãƒ 

ã“ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€ãƒ†ãƒ‹ã‚¹å‹•ç”»ãƒ»ç”»åƒã‹ã‚‰åŒ…æ‹¬çš„ãªè§£æã‚’è¡Œã†
çµ±åˆGradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ã™ã€‚æœ€æ–°ã®predictor APIã‚’æ´»ç”¨ã—ã€
ãƒœãƒ¼ãƒ«æ¤œå‡ºã€ã‚³ãƒ¼ãƒˆè§£æã€ãƒ—ãƒ¬ãƒ¼ãƒ¤ãƒ¼è¿½è·¡ã€ã‚¤ãƒ™ãƒ³ãƒˆåˆ†æã¾ã§ã‚’
ãƒ¯ãƒ³ã‚¹ãƒˆãƒƒãƒ—ã§æä¾›ã—ã¾ã™ã€‚

Features:
- ğŸ¯ ãƒœãƒ¼ãƒ«æ¤œå‡ºãƒ»è»Œè·¡å¯è¦–åŒ–
- ğŸŸï¸ ã‚³ãƒ¼ãƒˆèªè­˜ãƒ»ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ¤œå‡º
- ğŸ‘¥ ãƒ—ãƒ¬ãƒ¼ãƒ¤ãƒ¼æ¤œå‡ºãƒ»å§¿å‹¢æ¨å®š
- ğŸ“Š çµ±è¨ˆåˆ†æãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
- âš¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ»ãƒãƒƒãƒå‡¦ç†
- ğŸ¨ é«˜å“è³ªå¯è¦–åŒ–ãƒ»ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

Author: Tennis Analysis System Team
License: MIT
Version: 1.0.0
"""

import gradio as gr
import torch
import numpy as np
import cv2
import json
import time
import os
import sys
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional, Union
from PIL import Image
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from src.predictor import (
        VideoPipeline,
        create_ball_detector,
        VisualizationConfig,
        HIGH_PERFORMANCE_CONFIG,
        MEMORY_EFFICIENT_CONFIG,
        REALTIME_CONFIG,
        DEBUG_CONFIG
    )
    PREDICTOR_AVAILABLE = True
except ImportError as e:
    PREDICTOR_AVAILABLE = False
    PREDICTOR_ERROR = str(e)
    print(f"Warning: Predictor system not available: {e}")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DEFAULT_MODEL_PATHS = {
    "ball_lite_tracknet": "checkpoints/ball/lite_tracknet.ckpt",
    "ball_wasb_sbdt": "checkpoints/ball/wasb_sbdt.pth", 
    "court": "checkpoints/court/lite_tracknet.ckpt",
    "player": "checkpoints/player/rt_detr.ckpt"
}

# ====================================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ====================================================================

def get_available_models() -> Dict[str, List[str]]:
    """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
    available = {"ball": [], "court": [], "player": []}
    
    for model_type, default_path in DEFAULT_MODEL_PATHS.items():
        if os.path.exists(default_path):
            category = model_type.split("_")[0]
            available[category].append(default_path)
    
    # checkpointsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢
    checkpoints_dir = Path("checkpoints")
    if checkpoints_dir.exists():
        for category in available.keys():
            category_dir = checkpoints_dir / category
            if category_dir.exists():
                for ext in ["*.ckpt", "*.pth"]:
                    available[category].extend([str(p) for p in category_dir.glob(ext)])
    
    # é‡è¤‡é™¤å»
    for category in available:
        available[category] = list(set(available[category]))
    
    return available

def create_pipeline_config(config_name: str, custom_params: Dict[str, Any] = None) -> Dict[str, Any]:
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­å®šä½œæˆ"""
    base_configs = {
        "high_performance": HIGH_PERFORMANCE_CONFIG,
        "memory_efficient": MEMORY_EFFICIENT_CONFIG,
        "realtime": REALTIME_CONFIG,
        "debug": DEBUG_CONFIG
    }
    
    config = base_configs.get(config_name, MEMORY_EFFICIENT_CONFIG).copy()
    
    if custom_params:
        config.update(custom_params)
    
    return config

def create_visualization_config(**kwargs) -> VisualizationConfig:
    """å¯è¦–åŒ–è¨­å®šä½œæˆ"""
    return VisualizationConfig(**kwargs)

def generate_statistics_report(results: Dict[str, Any]) -> Dict[str, Any]:
    """çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    report = {
        "processing_info": {
            "total_frames": results.get("total_frames", 0),
            "processing_time": results.get("processing_time", 0.0),
            "average_fps": results.get("average_fps", 0.0)
        },
        "detection_stats": {},
        "analysis_summary": {}
    }
    
    # ãƒœãƒ¼ãƒ«æ¤œå‡ºçµ±è¨ˆ
    detections = results.get("detections", {})
    if detections:
        total_detections = sum(len(det_list) for det_list in detections.values())
        frames_with_detection = sum(1 for det_list in detections.values() if det_list)
        
        report["detection_stats"] = {
            "total_detections": total_detections,
            "frames_with_detection": frames_with_detection,
            "detection_rate": frames_with_detection / len(detections) if detections else 0.0
        }
    
    return report

def create_trajectory_plot(detections: Dict[str, List]) -> go.Figure:
    """è»Œè·¡ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ"""
    fig = go.Figure()
    
    x_coords = []
    y_coords = []
    frame_nums = []
    
    for frame_id, det_list in detections.items():
        if det_list:
            # ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·ã‚’æŠ½å‡º
            frame_num = int(frame_id.replace("frame_", ""))
            for det in det_list:
                if len(det) >= 2:
                    x_coords.append(det[0])
                    y_coords.append(det[1])
                    frame_nums.append(frame_num)
    
    if x_coords:
        fig.add_trace(go.Scatter(
            x=x_coords,
            y=y_coords,
            mode='lines+markers',
            name='Ball Trajectory',
            line=dict(color='red', width=2),
            marker=dict(size=4)
        ))
        
        fig.update_layout(
            title="Tennis Ball Trajectory",
            xaxis_title="X Coordinate (normalized)",
            yaxis_title="Y Coordinate (normalized)",
            template="plotly_white"
        )
    
    return fig

# ====================================================================
# ãƒœãƒ¼ãƒ«æ¤œå‡ºã‚¿ãƒ–
# ====================================================================

def analyze_ball_detection(
    video_file: str,
    model_path: str,
    model_type: str,
    config_preset: str,
    ball_radius: int,
    trajectory_length: int,
    enable_smoothing: bool,
    enable_prediction: bool,
    confidence_threshold: float,
    progress=gr.Progress()
) -> Tuple[str, str, go.Figure]:
    """ãƒœãƒ¼ãƒ«æ¤œå‡ºè§£æå®Ÿè¡Œ"""
    if not PREDICTOR_AVAILABLE:
        raise gr.Error(f"Predictor system not available: {PREDICTOR_ERROR}")
    
    if not video_file:
        raise gr.Error("å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
    
    if not os.path.exists(model_path):
        raise gr.Error(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
    
    progress(0.1, desc="åˆæœŸåŒ–ä¸­...")
    
    try:
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­å®š
        pipeline_config = create_pipeline_config(config_preset)
        pipeline = VideoPipeline(pipeline_config)
        
        # æ¤œå‡ºå™¨è¨­å®š
        detector_config = {
            "model_path": model_path,
            "model_type": model_type,
            "device": DEVICE
        }
        
        # å¯è¦–åŒ–è¨­å®š
        vis_config = create_visualization_config(
            ball_radius=ball_radius,
            trajectory_length=trajectory_length,
            enable_smoothing=enable_smoothing,
            enable_prediction=enable_prediction,
            confidence_threshold=confidence_threshold
        )
        
        progress(0.2, desc="å‡¦ç†é–‹å§‹...")
        
        # å‹•ç”»å‡¦ç†å®Ÿè¡Œ
        output_filename = f"ball_detection_{int(time.time())}.mp4"
        result = pipeline.process_video(
            video_path=video_file,
            detector_config=detector_config,
            output_path=output_filename,
            vis_config=vis_config
        )
        
        progress(0.9, desc="çµ±è¨ˆç”Ÿæˆä¸­...")
        
        # çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        stats_report = generate_statistics_report(result)
        stats_json = json.dumps(stats_report, indent=2, ensure_ascii=False)
        
        # è»Œè·¡ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
        trajectory_plot = create_trajectory_plot(result.get("detections", {}))
        
        progress(1.0, desc="å®Œäº†!")
        
        return output_filename, stats_json, trajectory_plot
        
    except Exception as e:
        raise gr.Error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

def create_ball_detection_tab() -> gr.Tab:
    """ãƒœãƒ¼ãƒ«æ¤œå‡ºã‚¿ãƒ–ä½œæˆ"""
    with gr.Tab("ğŸ¯ Ball Detection", id="ball_detection") as tab:
        gr.Markdown("""
        ## ãƒ†ãƒ‹ã‚¹ãƒœãƒ¼ãƒ«æ¤œå‡ºãƒ»è»Œè·¡è§£æ
        
        å‹•ç”»ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€AIã«ã‚ˆã‚‹ãƒœãƒ¼ãƒ«æ¤œå‡ºã¨è»Œè·¡å¯è¦–åŒ–ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
        é«˜ç²¾åº¦ãªæ¤œå‡ºã¨æ»‘ã‚‰ã‹ãªè»Œè·¡è¡¨ç¤ºã€çµ±è¨ˆåˆ†æã‚’æä¾›ã—ã¾ã™ã€‚
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“ Input Settings")
                
                video_input = gr.File(
                    label="å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«",
                    file_types=["video"],
                    type="filepath"
                )
                
                available_models = get_available_models()
                model_path = gr.Dropdown(
                    label="ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«",
                    choices=available_models["ball"] if available_models["ball"] else ["No models found"],
                    value=available_models["ball"][0] if available_models["ball"] else None
                )
                
                model_type = gr.Radio(
                    label="ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—",
                    choices=["auto", "lite_tracknet", "wasb_sbdt"],
                    value="auto"
                )
                
                gr.Markdown("### âš™ï¸ Processing Settings")
                
                config_preset = gr.Radio(
                    label="å‡¦ç†è¨­å®šãƒ—ãƒªã‚»ãƒƒãƒˆ",
                    choices=["high_performance", "memory_efficient", "realtime", "debug"],
                    value="memory_efficient"
                )
                
                gr.Markdown("### ğŸ¨ Visualization Settings")
                
                ball_radius = gr.Slider(
                    label="ãƒœãƒ¼ãƒ«æç”»åŠå¾„",
                    minimum=4, maximum=20, value=8, step=1
                )
                
                trajectory_length = gr.Slider(
                    label="è»Œè·¡è¡¨ç¤ºãƒ•ãƒ¬ãƒ¼ãƒ æ•°",
                    minimum=5, maximum=50, value=20, step=1
                )
                
                enable_smoothing = gr.Checkbox(
                    label="ä½ç½®ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°æœ‰åŠ¹åŒ–",
                    value=False
                )
                
                enable_prediction = gr.Checkbox(
                    label="ä½ç½®äºˆæ¸¬è¡¨ç¤ºæœ‰åŠ¹åŒ–",
                    value=False
                )
                
                confidence_threshold = gr.Slider(
                    label="ä¿¡é ¼åº¦é–¾å€¤",
                    minimum=0.1, maximum=1.0, value=0.5, step=0.05
                )
                
                submit_btn = gr.Button(
                    "ğŸš€ è§£æé–‹å§‹",
                    variant="primary",
                    size="lg"
                )
            
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ“Š Results")
                
                with gr.Row():
                    output_video = gr.Video(
                        label="å‡¦ç†æ¸ˆã¿å‹•ç”»",
                        height=400
                    )
                
                with gr.Row():
                    with gr.Column():
                        stats_output = gr.JSON(
                            label="çµ±è¨ˆæƒ…å ±"
                        )
                    
                    with gr.Column():
                        trajectory_plot = gr.Plot(
                            label="è»Œè·¡ãƒ—ãƒ­ãƒƒãƒˆ"
                        )
        
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©
        submit_btn.click(
            fn=analyze_ball_detection,
            inputs=[
                video_input, model_path, model_type, config_preset,
                ball_radius, trajectory_length, enable_smoothing,
                enable_prediction, confidence_threshold
            ],
            outputs=[output_video, stats_output, trajectory_plot]
        )
    
    return tab

# ====================================================================
# ãƒãƒƒãƒå‡¦ç†ã‚¿ãƒ–
# ====================================================================

def run_batch_processing(
    input_files: List[str],
    model_path: str,
    model_type: str,
    config_preset: str,
    parallel_jobs: int,
    continue_on_error: bool,
    progress=gr.Progress()
) -> Tuple[str, str]:
    """ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ"""
    if not PREDICTOR_AVAILABLE:
        raise gr.Error(f"Predictor system not available: {PREDICTOR_ERROR}")
    
    if not input_files:
        raise gr.Error("å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
    
    progress(0.1, desc="ãƒãƒƒãƒå‡¦ç†æº–å‚™ä¸­...")
    
    try:
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir = f"batch_output_{int(time.time())}"
        os.makedirs(output_dir, exist_ok=True)
        
        # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œï¼ˆç°¡å˜ãªå®Ÿè£…ï¼‰
        results = []
        total_files = len(input_files)
        
        for i, video_file in enumerate(input_files):
            progress((i + 1) / total_files, desc=f"å‡¦ç†ä¸­ ({i+1}/{total_files}): {os.path.basename(video_file)}")
            
            try:
                # å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
                pipeline_config = create_pipeline_config(config_preset)
                pipeline = VideoPipeline(pipeline_config)
                
                detector_config = {
                    "model_path": model_path,
                    "model_type": model_type,
                    "device": DEVICE
                }
                
                output_filename = os.path.join(
                    output_dir, 
                    f"{Path(video_file).stem}_annotated.mp4"
                )
                
                result = pipeline.process_video(
                    video_path=video_file,
                    detector_config=detector_config,
                    output_path=output_filename
                )
                
                results.append({
                    "file": video_file,
                    "status": "success",
                    "output": output_filename,
                    "stats": result
                })
                
            except Exception as e:
                error_msg = str(e)
                results.append({
                    "file": video_file,
                    "status": "error",
                    "error": error_msg
                })
                
                if not continue_on_error:
                    break
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        batch_report = {
            "summary": {
                "total_files": len(input_files),
                "processed_files": sum(1 for r in results if r["status"] == "success"),
                "failed_files": sum(1 for r in results if r["status"] == "error"),
            },
            "detailed_results": results
        }
        
        report_json = json.dumps(batch_report, indent=2, ensure_ascii=False)
        
        # ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆç°¡å˜ãªå®Ÿè£…ï¼‰
        import shutil
        zip_filename = f"{output_dir}.zip"
        shutil.make_archive(output_dir, 'zip', output_dir)
        
        progress(1.0, desc="å®Œäº†!")
        
        return zip_filename, report_json
        
    except Exception as e:
        raise gr.Error(f"ãƒãƒƒãƒå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

def create_batch_processing_tab() -> gr.Tab:
    """ãƒãƒƒãƒå‡¦ç†ã‚¿ãƒ–ä½œæˆ"""
    with gr.Tab("ğŸ“ Batch Processing", id="batch_processing") as tab:
        gr.Markdown("""
        ## ãƒãƒƒãƒå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
        
        è¤‡æ•°ã®å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬ã§å‡¦ç†ã—ã¾ã™ã€‚
        ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–ã¨ã‚¨ãƒ©ãƒ¼å›å¾©æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“ Input Settings")
                
                input_files = gr.File(
                    label="å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰",
                    file_count="multiple",
                    file_types=["video"],
                    type="filepath"
                )
                
                available_models = get_available_models()
                model_path = gr.Dropdown(
                    label="ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«",
                    choices=available_models["ball"] if available_models["ball"] else ["No models found"],
                    value=available_models["ball"][0] if available_models["ball"] else None
                )
                
                model_type = gr.Radio(
                    label="ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—",
                    choices=["auto", "lite_tracknet", "wasb_sbdt"],
                    value="auto"
                )
                
                gr.Markdown("### âš™ï¸ Batch Settings")
                
                config_preset = gr.Radio(
                    label="å‡¦ç†è¨­å®šãƒ—ãƒªã‚»ãƒƒãƒˆ",
                    choices=["high_performance", "memory_efficient", "realtime", "debug"],
                    value="memory_efficient"
                )
                
                parallel_jobs = gr.Slider(
                    label="ä¸¦åˆ—ã‚¸ãƒ§ãƒ–æ•°",
                    minimum=1, maximum=8, value=2, step=1
                )
                
                continue_on_error = gr.Checkbox(
                    label="ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚å‡¦ç†ç¶™ç¶š",
                    value=True
                )
                
                batch_submit_btn = gr.Button(
                    "ğŸš€ ãƒãƒƒãƒå‡¦ç†é–‹å§‹",
                    variant="primary",
                    size="lg"
                )
            
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ“Š Results")
                
                output_zip = gr.File(
                    label="å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆZIPï¼‰"
                )
                
                batch_report = gr.JSON(
                    label="ãƒãƒƒãƒå‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆ"
                )
        
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©
        batch_submit_btn.click(
            fn=run_batch_processing,
            inputs=[
                input_files, model_path, model_type, config_preset,
                parallel_jobs, continue_on_error
            ],
            outputs=[output_zip, batch_report]
        )
    
    return tab

# ====================================================================
# è¨­å®šãƒ»æƒ…å ±ã‚¿ãƒ–
# ====================================================================

def get_system_info() -> Dict[str, Any]:
    """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—"""
    return {
        "device": DEVICE,
        "torch_version": torch.__version__,
        "cuda_available": torch.cuda.is_available(),
        "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
        "predictor_available": PREDICTOR_AVAILABLE,
        "available_models": get_available_models()
    }

def create_settings_tab() -> gr.Tab:
    """è¨­å®šãƒ»æƒ…å ±ã‚¿ãƒ–ä½œæˆ"""
    with gr.Tab("âš™ï¸ Settings & Info", id="settings") as tab:
        gr.Markdown("""
        ## ã‚·ã‚¹ãƒ†ãƒ è¨­å®šãƒ»æƒ…å ±
        
        ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®ç¢ºèªã¨ãƒ¢ãƒ‡ãƒ«ç®¡ç†ã‚’è¡Œã„ã¾ã™ã€‚
        """)
        
        with gr.Row():
            with gr.Column():
                gr.Markdown("### ğŸ’» System Information")
                
                system_info = gr.JSON(
                    label="ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±",
                    value=get_system_info()
                )
                
                refresh_btn = gr.Button("ğŸ”„ æƒ…å ±æ›´æ–°")
                
                gr.Markdown("### ğŸ“‹ Available Models")
                
                models_info = gr.JSON(
                    label="åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«",
                    value=get_available_models()
                )
            
            with gr.Column():
                gr.Markdown("### ğŸ“š Usage Guide")
                
                gr.Markdown("""
                #### åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•
                
                1. **Ball Detection**ã‚¿ãƒ–ã§å˜ä¸€å‹•ç”»è§£æ
                2. **Batch Processing**ã‚¿ãƒ–ã§è¤‡æ•°å‹•ç”»ä¸€æ‹¬å‡¦ç†
                3. å„ã‚¿ãƒ–ã§é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
                4. å‡¦ç†è¨­å®šã‚’èª¿æ•´ï¼ˆé«˜æ€§èƒ½ vs ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ï¼‰
                5. å¯è¦–åŒ–è¨­å®šã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
                
                #### æ¨å¥¨è¨­å®š
                
                - **é«˜é€Ÿå‡¦ç†**: high_performance preset
                - **ãƒ¡ãƒ¢ãƒªç¯€ç´„**: memory_efficient preset  
                - **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ **: realtime preset
                - **ãƒ‡ãƒãƒƒã‚°**: debug preset
                
                #### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
                
                - GPUä½¿ç”¨æ™‚ã¯CUDAç’°å¢ƒã‚’ç¢ºèª
                - ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãŒæ­£ã—ã„ã“ã¨ã‚’ç¢ºèª
                - ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã¯ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
                """)
        
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©
        refresh_btn.click(
            fn=lambda: (get_system_info(), get_available_models()),
            outputs=[system_info, models_info]
        )
    
    return tab

# ====================================================================
# ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
# ====================================================================

def create_main_app() -> gr.Blocks:
    """ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ"""
    
    # ã‚«ã‚¹ã‚¿ãƒ CSS
    custom_css = """
    .gradio-container {
        max-width: 1400px !important;
    }
    .tab-nav {
        border-bottom: 2px solid #e1e5e9;
    }
    .tennis-header {
        text-align: center;
        background: linear-gradient(90deg, #4CAF50, #2196F3);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        font-size: 2.5rem;
        font-weight: bold;
        margin-bottom: 1rem;
    }
    """
    
    with gr.Blocks(
        title="Tennis Analysis System",
        css=custom_css,
        theme=gr.themes.Soft()
    ) as app:
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        gr.Markdown("""
        <h1 class="tennis-header">ğŸ¾ Tennis Analysis System</h1>
        <p style="text-align: center; font-size: 1.2rem; color: #666;">
        AI-Powered Tennis Video Analysis Platform
        </p>
        """)
        
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç¢ºèª
        if not PREDICTOR_AVAILABLE:
            gr.Markdown(f"""
            <div style="background-color: #ffe6e6; padding: 1rem; border-radius: 8px; border-left: 4px solid #ff4444;">
            <strong>âš ï¸ Warning:</strong> Predictor system is not available.<br>
            <strong>Error:</strong> {PREDICTOR_ERROR}<br>
            Some features may be limited.
            </div>
            """)
        
        # ã‚¿ãƒ–ä½œæˆ
        with gr.Tabs():
            ball_tab = create_ball_detection_tab()
            batch_tab = create_batch_processing_tab()
            settings_tab = create_settings_tab()
        
        # ãƒ•ãƒƒã‚¿ãƒ¼
        gr.Markdown("""
        ---
        <p style="text-align: center; color: #888; font-size: 0.9rem;">
        Tennis Analysis System v1.0.0 | Powered by PyTorch, Gradio, and WASB-SBDT
        </p>
        """)
    
    return app

# ====================================================================
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•
# ====================================================================

def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ"""
    print("ğŸ¾ Tennis Analysis System Starting...")
    print(f"Device: {DEVICE}")
    print(f"Predictor Available: {PREDICTOR_AVAILABLE}")
    
    app = create_main_app()
    
    # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=False,
        show_error=True,
        quiet=False
    )

if __name__ == "__main__":
    main() 